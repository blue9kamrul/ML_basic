{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLeEX5ZHHQGxkarqaKrwZC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"lVkjxxI7Rz-1","executionInfo":{"status":"ok","timestamp":1751194272813,"user_tz":-360,"elapsed":45,"user":{"displayName":"Kamrul Islam","userId":"03739884832583667358"}}},"outputs":[],"source":["# at first save the html page i want to scrape, because web might not allow to extract everything from same ip\n","# so use proxy like dataimpulse.com which acts as the mediatary between client and server\n"]},{"cell_type":"code","source":["# 0 method / retrieving html\n","\n","import requests\n","from bs4 import BeautifulSoup"],"metadata":{"id":"9CH8671PUAFv","executionInfo":{"status":"ok","timestamp":1751194298959,"user_tz":-360,"elapsed":127,"user":{"displayName":"Kamrul Islam","userId":"03739884832583667358"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["url = \"https://www.flipkart.com/audio-video/headset/pr?sid=0pm%2Cfcn$5B\"\n","\n","r = requests.get(url)\n","print(r.text)\n","\n","with open(\"file.html\", \"w\") as f:\n","    f.write(r.text)\n","\n","#here checks are you human, so problems\n","# so will use proxy and time delay"],"metadata":{"id":"LQ1fTcRlUGem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","from fake_useragent import UserAgent\n","\n","url = '......'\n","\n","session = requests.Session()\n","headers={\n","    'user-agent': UserAgent().random,\n","    'Accept-Language': 'en-US,en;q=0.9',\n","    'Accept-Encoding': 'gzip, deflate, br',\n","    'Connection': 'keep-alive',\n","    'Referer': 'https://www.google.com/',\n","    # 'Upgrade-Insecure-Requests': '1',\n","    # 'Sec-Fetch-Dest': 'document',\n","    # 'Sec-Fetch-Mode': 'navigate',\n","    # 'Sec-Fetch-Site': 'same-origin,\n","\n","}\n","\n","proxy_auth = '..from dataimpulse copy'\n","\n","proxies = {\n","    'http': f'http://{proxy_auth}',\n","    'https': f'https://{proxy_auth}'\n","}\n","# at network we can find headers\n","# we need to fake headers\n","# even after session we are getting are u human\n","# so use=ing time delay and use proxy\n","time.sleep(2)\n","r = session.get(url, proxies = proxies, headers = headers)\n","#now it works, although css is not working\n"],"metadata":{"id":"q_JYp8JGXSW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now will loop through and extract data single page by page\n"],"metadata":{"id":"2Q4pjDzUZZ6x"},"execution_count":null,"outputs":[]}]}