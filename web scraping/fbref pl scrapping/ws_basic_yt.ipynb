{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BpPw_exhmjC",
        "outputId": "f96e3364-299d-4fc4-9467-723b95f6da47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWjpLqGIw449",
        "outputId": "27afc40f-0aba-420d-f33f-67a72ca58f1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "FA-Os3MhxHyK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('home.html', 'r') as html_file:\n",
        "    content = html_file.read()\n",
        "    print(content)\n",
        "    soup = BeautifulSoup(content, 'lxml')\n",
        "    print(soup.prettify())\n",
        "    tags = soup.find('h5')\n",
        "    print(tags) #print full tag\n",
        "    # find method searches for firdt element and stop searching\n",
        "    # find_all searches all the tags there are\n",
        "    courses_html_tags = soup.find_all('h5')\n",
        "    # here we found all the tags so iterate over all the tags\n",
        "    for course in courses_html_tags:\n",
        "        print(course.text) # .text gives the text of that element\n",
        "\n",
        "# here we only scraped the basic local html file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LWJRikpCxQAr",
        "outputId": "6087a9b8-3f43-4b3e-d521-b485959ff34b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'home.html'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1651066213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'home.html'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhtml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'home.html'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now scraping from the web with inspect tool\n",
        "with open ('home.html', 'r') as html_file:\n",
        "    content = html_file.read()\n",
        "    print(content)\n",
        "    soup = BeautifulSoup(content, 'lxml')\n",
        "    course_cards = soup.find_all('div', class_='card') #as class is a python built in method so we need an underscre to distinguish\n",
        "    for course in course_cards:\n",
        "        course_name = course.h5.text\n",
        "        course_price = course.a.text.split()[-1] #here we are only taking the last word of the a.text by splitting and taking the last one\n",
        "\n",
        "        print(f'{course_name} costs {course_price}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "AoqpK9CuxoOp",
        "outputId": "01f1bdb3-c518-4e21-e0b6-af220b675a7b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'home.html'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-186897198.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now scraping from the web with inspect tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'home.html'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhtml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'home.html'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now scrape real website with requests lib\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES_-p40s3IQo",
        "outputId": "8f8e1169-6cb1-49fd-d0d7-a546865b618b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "ZY2-OWEr3WQ2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
        "soup = BeautifulSoup(html_text, 'lxml')\n",
        "\n",
        "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
        "\n",
        "with open('python_jobs.txt', 'w') as f:\n",
        "    for job in jobs:\n",
        "        company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
        "        skills_element = job.find('span', class_='srp-skills')\n",
        "        published_date_element = job.find('span', class_='sim-posted')\n",
        "\n",
        "        if skills_element and published_date_element:\n",
        "            skills = skills_element.text.replace(' ', '')\n",
        "            published_date = published_date_element.span.text\n",
        "\n",
        "            if 'few' in published_date:\n",
        "                f.write(f\"Company Name: {company_name.strip()}\\n\")\n",
        "                f.write(f\"Required Skills: {skills.strip()}\\n\")\n",
        "                f.write(f\"Published Date: {published_date.strip()}\\n\")\n",
        "                f.write(\"-\" * 20 + \"\\n\")\n",
        "        else:\n",
        "            # Optionally, handle cases where elements are not found\n",
        "            pass\n",
        "    print(company_name,skills_element,published_date_element)\n",
        "print(\"Job information saved to python_jobs.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q1fmhtgH7w2e",
        "outputId": "5bf2e39e-798c-4b41-9c75-8af7ec7e1613"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\t\trealizertechnologies\r\n",
            "\t\t\r\n",
            "\t\t None <span class=\"sim-posted\">\n",
            "<span>few days ago</span>\n",
            "</span>\n",
            "Job information saved to python_jobs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "0QTBRrBA8sbD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unfamiliar_skill = input('>Enter the skill you are not familiar with: ')\n",
        "print(f'Filtering out {unfamiliar_skill}')\n",
        "import time\n",
        "\n",
        "def find_jobs():\n",
        "    html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
        "\n",
        "    for index, job in enumerate(jobs):\n",
        "        published_date = job.find('span', class_='sim-posted').span.text\n",
        "        if 'few' in published_date:\n",
        "            company_name = job.find('h3', class_='joblist-comp-name').text.replace(' ', '')\n",
        "            skills = job.find('span', class_='srp-skills')\n",
        "            more_info = job.header.h2.a['href']\n",
        "\n",
        "            if skills:\n",
        "                skills = skills.text.replace(' ', '')\n",
        "            else:\n",
        "                skills = 'N/A'\n",
        "            if unfamiliar_skill not in skills:\n",
        "                with open(f'posts/{index}.txt', 'w') as f:\n",
        "                    f.write(f\"Company Name: {company_name.strip()}\\n\")\n",
        "                    f.write(f\"Required Skills: {skills.strip()}\\n\")\n",
        "                    f.write(f\"Published Date: {published_date.strip()}\\n\")\n",
        "\n",
        "                    print(f\"Company name: {company_name.strip()}\")\n",
        "                    print(f\"Required skills: {skills}\")\n",
        "                    print(f\"Published date: {published_date}\")\n",
        "                    print(f\"More info: {more_info}\")\n",
        "            print(f'file saved{index}')\n",
        "            print('')\n",
        "\n",
        "            #here maybe the class or something of the given website changed. And as a result skills are not captured\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    while True:\n",
        "        find_jobs()\n",
        "        time_wait = 10\n",
        "        print(f'Waiting {time_wait} minutes...')\n",
        "        time.sleep(time_wait * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8DDus8oGWoK",
        "outputId": "330e5fef-aec4-4f32-dbd5-053650b9c05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">Enter the skill you are not familiar with: sdfsdd\n",
            "Filtering out sdfsdd\n",
            "Company name: CONNECTING2WORK\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-connecting-2-work-bengaluru-bangalore-5-to-8-yrs-jobid-MF2GI0PP7HhzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved9\n",
            "\n",
            "Company name: CONNECTING2WORK\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-connecting-2-work-cochin-kochi-ernakulam-2-to-5-yrs-jobid-75__PLUS__IuvTDc__SLASH__ZzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved10\n",
            "\n",
            "Company name: CONNECTING2WORK\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-trainer-connecting-2-work-calicut-kozhikode-0-to-3-yrs-jobid-Z__PLUS__uCQzJQBSxzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved11\n",
            "\n",
            "Company name: CONNECTING2WORK\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-connecting-2-work-bengaluru-bangalore-0-to-3-yrs-jobid-DUXY1__PLUS__Mqw8FzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved12\n",
            "\n",
            "Company name: ORACLE\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-oracle-hyderabad-secunderabad-2-to-5-yrs-jobid-eg__SLASH__2lRco48pzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved13\n",
            "\n",
            "Company name: SEVENCONSULTANCY\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-seven-consultancy-surat-9-to-12-yrs-jobid-Kdex__PLUS__SlZxqJzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved14\n",
            "\n",
            "Company name: ExcelPtp\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-trainer-excel-ptp-ahmedabad-1-to-4-yrs-jobid-gD8e3t5QaQVzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved15\n",
            "\n",
            "Company name: CtdTechsPrivateLimited\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-ctd-techs-private-limited-chennai-bengaluru-bangalore-3-to-5-yrs-jobid-m__SLASH__IX__SLASH__aNQuAZzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved16\n",
            "\n",
            "Company name: botreetechnologies\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developers-botree-technologies-ahmedabad-3-to-6-yrs-jobid-QHaMntpZqO9zpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved17\n",
            "\n",
            "Company name: highrisesolutionsllp\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-highrise-solutions-llp-pune-4-to-7-yrs-jobid-R5__SLASH__LnD1RVwxzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved18\n",
            "\n",
            "Company name: 3RITechnologiesPvtLtd\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-programmer-3ri-technologies-pvt-ltd-pune-0-to-1-yrs-jobid-RisAg8NKGbBzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved19\n",
            "\n",
            "Company name: ADROITLEARNINGANDMANPOWERPVTLTD\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-adroit-learning-and-manpower-pvt-ltd-ahmedabad-0-to-5-yrs-jobid-Gisc8S6O1sBzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved20\n",
            "\n",
            "Company name: AxisTechnolabs\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-axistechnolabs-ahmedabad-0-to-1-yrs-jobid-oQxb__SLASH__TVWjxlzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved21\n",
            "\n",
            "Company name: TechasoftPvtLtd\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-techasoft-pvt-ltd-bengaluru-bangalore-0-to-3-yrs-jobid-a7yPAxSxElFzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved22\n",
            "\n",
            "Company name: SEVENCONSULTANCY\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-seven-consultancy-navi-mumbai-mumbai-4-to-7-yrs-jobid-VJ3GBkmUwLVzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved23\n",
            "\n",
            "Company name: realizertechnologies\n",
            "Required skills: N/A\n",
            "Published date: few days ago\n",
            "More info: https://www.timesjobs.com/job-detail/python-developer-realizer-technologies-pune-0-to-3-yrs-jobid-F362LGN73BZzpSvf__PLUS__uAgZw==&source=srp\n",
            "file saved24\n",
            "\n",
            "Waiting 10 minutes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D46og5leGdin"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}